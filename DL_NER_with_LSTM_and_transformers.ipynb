{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iduryodhanrao/ml-tests/blob/main/DL_NER_with_LSTM_and_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlzUBjlgazPB"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4OSGXCcKUqN_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rQMEE1PDxui"
      },
      "source": [
        "**NER prediction with LSTM and Transformers**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xbfr2JbWUDwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4viZuySLD3HD"
      },
      "source": [
        "For this assignment we will be exploring the use of lstms and transformers for named entity recognition (NER) tasks. In this case, we will be looking at recognizing word tagging (e.g., classifying each word as a business, a place, etc...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z8dD6pSEXDm"
      },
      "source": [
        "First, download and upload the ner_dataset.csv file from this site (https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus?select=ner_dataset.csv), we will be using this for experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-Zvwl90EnZJ"
      },
      "source": [
        "Import the libraries we will need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wFWdYWwEqFp"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from itertools import chain\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRyutDB9E0sX"
      },
      "source": [
        "Let's look at the structure of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "yNj70CojE3Ve",
        "outputId": "cd42c34a-0c2a-46f4-d253-ea6d70d585de"
      },
      "source": [
        "data = pd.read_csv('ner_dataset.csv', encoding= 'unicode_escape')\n",
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'ner_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-72e36543b354>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ner_dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'unicode_escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ner_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jvynmAHFjKx"
      },
      "source": [
        "We next need to create a mapping between tokens, tags, and ids. Each token should map to a unique id, and each tag should map to a unique class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSg1neW0IdnD"
      },
      "source": [
        "Now you might have noticed that each sentece is split into multiple rows.\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "We need to transform this data into sequences of words and tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibNA2AqWZp4Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "94a80fb7-8c54-4b6b-d199-03542e0e4714"
      },
      "source": [
        "# Fill na\n",
        "data_fillna = data.fillna(method='ffill', axis=0)\n",
        "# Groupby and collect columns\n",
        "data_group = data_fillna.groupby(\n",
        "['Sentence #'],as_index=False\n",
        ")['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n",
        "# Visualise data\n",
        "data_group.head(10).\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-5-a27a943f7aea>, line 8)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-a27a943f7aea>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    data_group.head(10).\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Jske89drZgXA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oBPyo8ZJDPm"
      },
      "source": [
        "Next we split the data into training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoT2bYY1ZsNg"
      },
      "source": [
        "#Enter your code here\n",
        "# sample data\n",
        "['thousands of demonstrators were in Hyde park', 'thousands', 'O']\n",
        "# ['thousands of demonstrators were in Hyde park', 'of', 'O']\n",
        "['thousands of demonstrators were in Hyde park', 'demonstrators', 'O']\n",
        "\n",
        "['thousands of demonstrators were in Hyde park', 'Hyde', 'A-geo']\n",
        "['thousands of demonstrators were in Hyde park', 'Park', 'B-geo']\n",
        "\n",
        "# X, y\n",
        "['thousands of demonstrators were in Hyde park', 'thousands'] ['O']\n",
        "['thousands of demonstrators were in Hyde park', 'Hyde'] ['A-geo']\n",
        "\n",
        "# 100 samples of class 'O', 10 from 'A-geo', 10 from 'B-geo'\n",
        "# y - can take a few values. M classes\n",
        "# Multi class classification problem\n",
        "# CLass imbalance: O is super popular\n",
        "# Train M binary classifiers, each predicting i th class vs all\n",
        "# training data for class 'A-geo' : 10 vs 110\n",
        "# Need to down sample majority class\n",
        "[0.2, 0.1, 0.7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Metrics\n",
        "# Class specific PR => PR, ROC AUC"
      ],
      "metadata": {
        "id": "eLfmjjDPaALU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gkZ6PauuaoXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding:\n",
        "## Goal:\n",
        "'thousands of demonstrators were in Hyde park' =>\n",
        "[0.2, 1.1, 3.0, 0.4, ...0.9]\n",
        "\n",
        "## Approaches:\n",
        "\n",
        "\n",
        "1.   Word encoding: create a vocabulary. enumerate them\n",
        "10K - 20K words.\n",
        "\n",
        "thousands => 345\n",
        "thousand => 999\n",
        "demonstrators => 12\n",
        "demonstrate => 671\n",
        "demonstrating => 910\n",
        "\n",
        "one hot encoding for each token\n",
        "[]\n",
        "10 * 10K element sparse vector\n",
        "\n",
        "2.   Word piece encoding\n",
        "thousand => thou san d' '\n",
        "thousands => thou san ds' '\n",
        "repeat the same one-hot encoding\n",
        "\n",
        "hundreds\n",
        "\n",
        "3. Byte pair encoding\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3ayR8S9Maot5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HKC2MdNseQbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5AcJAYFEeQYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "['thousands of demonstrators were in Hyde park', 'thousands'] ['O'] =>\n",
        "[10, 31, 671, ..., 0]\n",
        "[10, 31, 671, ..., 2]"
      ],
      "metadata": {
        "id": "--BLfccdeLlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPBi9B3vKPfy"
      },
      "source": [
        "**Next** create the LSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Dxh27TQqeTdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_loaded_embeddings = {'sentence': [0.1,0.3]}\n",
        "def get_embedding(sentence):\n",
        "  unknown_sentence_embedding = [0,0,0]\n",
        "  e = pre_loaded_embeddings.get(sentence,\n",
        "                                   unknown_sentence_embedding)\n",
        "  return torch.tensor(e)"
      ],
      "metadata": {
        "id": "WVwzmoQXfD0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg2UnsT0KRfx"
      },
      "source": [
        "# Multi class classification model\n",
        "class LSTMNerModel(nn.Module):\n",
        "  def __init__(self,embedding_dim, num_tags, hidden_dim=64):\n",
        "    super().__init__()\n",
        "    self.embedding = get_embedding\n",
        "    self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim)\n",
        "    self.linear = nn.Linear(in_features=hidden_dim, out_features=num_tags)\n",
        "    self.soft_max = nn.functional.log_softmax # (in, dim=1)\n",
        "\n",
        "  def forward(self, sentence, word_index):\n",
        "    embeddings = self.embedding(sentence)\n",
        "    lstm_out, lstm_hidden = self.lstm(embeddings)\n",
        "    # decide which lstm output to use\n",
        "    # one option: pick out the word_index\n",
        "    y = lstm_out[:, word_index]\n",
        "    y = self.linear(y)\n",
        "    tag_scores = self.soft_max(y, dim=1)\n",
        "    return tag_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joMehFqIK67E"
      },
      "source": [
        "Define the loss function for the task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh3V2M_eK9Un"
      },
      "source": [
        "# Enter code here\n",
        "# CELoss => Binary CE Loss\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "model = LSTMNerModel(...)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# tags should be of the same form as the tag_scores\n",
        "# [0, 0, 1]\n",
        "for epoch in range(epochs):\n",
        "  for sentence, word_position, tags in training_data:\n",
        "    tag_scores = model.forward(sentence, word_position)\n",
        "    # tag_scores = model(sentence, word_position)\n",
        "    loss = loss_function(tag_scores, tags)\n",
        "\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # compute metrics\n",
        "    # plot convergence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eKtXoWvhfrdL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IKXPtCLLKf9"
      },
      "source": [
        "Train the model. First, find some pre-trained embeddings to help us with the task...for example, you can find GloVe embeddings here https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stbwKuR5LPd9"
      },
      "source": [
        "def load_embeddings():\n",
        "  lines = open(\"glove.6B.100d.txt\", \"r\").readlines()\n",
        "\n",
        "  w2e = {}\n",
        "  for l in lines:\n",
        "    s = l.split(\" \")\n",
        "    word = s[0]\n",
        "    embedding = np.zeros( (1, len(s)-1))\n",
        "    for k, x in enumerate(s[1:]):\n",
        "      embedding[0,k] = float(x.strip())\n",
        "\n",
        "    w2e[word] = embedding\n",
        "\n",
        "  return w2e\n",
        "\n",
        "w2e = load_embeddings()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKui8Vdu74GG"
      },
      "source": [
        "# Enter Code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}