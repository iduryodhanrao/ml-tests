# -*- coding: utf-8 -*-
"""Copy of deep-learning-i-coding

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OISGSuDOME9_KWJyE_YtCZWkhcwuAy30
"""

# Install these specific versions of these libraries
!pip install pytorch_lightning==1.3.7.post0 ray[default]==1.5.1 tensorboard==2.4.1 tensorboardX==2.4

# Imports for this module
import pandas as pd
import numpy as np
import six
from typing import Union, List, Tuple

# Use wget to download these files
!wget https://github.com/amm-ik/ml-datasets/raw/main/module-4/bank-additional-full.csv
!wget https://github.com/amm-ik/ml-datasets/raw/main/module-4/bank-additional-names.txt

df = pd.read_csv("bank-additional-full.csv", sep=";")

df.sample(10)

df.info()

"""Lets take a look at the datasheet that accompanies our csv:"""

!sed -n '/^\s*Input variables/,/^8./p' bank-additional-names.txt

"""We see that there is mix of real values (numeric) and categorical values (strings).  Our classifier will not be able to handle these strings directly so we need to convert them. Models can only be trained on numbers.

In the following cell, encode these categorical values into numerical values so they can be used in a network.
"""

from sklearn.preprocessing import LabelEncoder

categoricals = [
    "job",
    "marital",
    "education",
    "housing",
    "loan",
    "contact",
    "poutcome",
    "y",
    "day_of_week",
    "poutcome",
    "default",
    "month",
]

def prep_data(df, cols):
    LE = LabelEncoder()
    for c in cols:
        df[c] = LE.fit_transform(df[c])
    return df

df = prep_data(df, categoricals)
df.head()

"""Note - all of this is performed on the full dataset. If we split the dataset into test and train before this, we would need to save the Label Encoders we use on the training set and apply them to the test set to ensure parity.

It can be easy to oversee or mismanage small things like this, so I recommend that you usually apply transformations like this over the entire dataset. Of course, there are exceptions, but this is a good rule of thumb for learning and exploring new models with pre-existing datasets.
"""

df[categoricals].sample(5)

"""Now, we will split the data into training and test data."""

def train_val_split(df, targetPct=0.80):
    # Group by the two categories of Y
    trainval = df.groupby(["y"], group_keys=False).apply(
        lambda f: f.sample(int(max(len(f) * targetPct, 2)))
    )

    # Split the train and test to have equal numbers of both y==1 and y==0
    test = df[~df.index.isin(trainval.index)]
    trainval_len, test_len, full_len = len(trainval), len(test), len(df)
    trainval_pop, test_pop, full_pop = (trainval["y"].values.mean(),test["y"].values.mean(), df["y"].values.mean(),)
    train_pct, test_pct = len(trainval) / len(df), len(test) / len(df)

    # Print our resulting train and test stratification details
    print("\n".join([
                f"Original Pop {full_pop:.2%} Count: {full_len}",
                f"TrainVal %: {train_pct:.2%}, TrainVal Pop: {trainval_pop:.2%} Count: {trainval_len}",
                f"Test %: {test_pct:.2%}, Test Pop: {test_pop:.2%} Count: {test_len}\n",
            ]))
    return trainval, test

# Stratified by Y using DataFrame.Sample
trainval, test = train_val_split(df, targetPct=0.90)

"""Instead of specifying a single validation set, lets use cross validation.  Write a function to assign validation folds the our trainval dataframe.  Again, make these folds stratified."""

from sklearn.model_selection import StratifiedKFold

def assign_kfold(trainval, n_splits=5):
    # Instantiate a new StratifiedKFold transformer
    spliter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=10)
    trainval["split"] = None

    # fold = (int) fold number n given by enumerate and n_splits
    # tr = training indicies for current split
    # te = testing indicies for current split
    for fold, (tr, te) in enumerate(spliter.split(trainval, trainval["y"])):
        trainval.iloc[te, trainval.columns.get_loc("split")] = fold

    # Calculate statistics on the splits to print and ensure are correct
    originalPop = trainval["y"].values.mean()
    originalLen = len(trainval)
    print(f"Original size: {originalLen}, Sampled % positive: {originalPop:.3f}")
    for k, grp in trainval.groupby("split"):
        pctSample = len(grp) / len(trainval)
        sampledPop = grp["y"].values.mean()
        print(
            f"Fold {k}: Fold Len: {len(grp)}, Fold Percentage {pctSample:.1%}, Fold % positive: {sampledPop:.1%}"
        )

    return trainval

# Create 4 splits
trainval = assign_kfold(trainval, n_splits=4)

trainval.head()

"""Now lets get into building and training our model.

Now that our data is preped and our environment is setup, we need to build a our model, as well as our training loop.

Lets start with our model.  In the next cells, we'll write code that defines our deep learning model.
"""

import ray
try: ray.init(log_to_driver=False)
except: pass

import torch
from torch import nn
from torch.utils.data import DataLoader
import pytorch_lightning as pl
from pytorch_lightning import loggers as pl_loggers
from pytorch_lightning.callbacks import LearningRateMonitor
from torch.utils.data import TensorDataset, DataLoader
from torchmetrics import (
    Accuracy,
    MetricCollection,
    Precision,
    Recall,
    AUROC,
    Specificity,
    AveragePrecision,
)


class MLP(pl.LightningModule):

    # Define a seven feature input and a full feature input to compare later
    input_map = {
        "seven": [
            "job",
            "marital",
            "education",
            "loan",
            "housing",
            "contact",
            "poutcome",
        ],
        "all": [
            "age",
            "job",
            "marital",
            "education",
            "default",
            "housing",
            "loan",
            "contact",
            "month",
            "day_of_week",
            "duration",
            "campaign",
            "pdays",
            "previous",
            "poutcome",
            "emp.var.rate",
            "cons.price.idx",
            "cons.conf.idx",
            "euribor3m",
            "nr.employed",
        ],
    }

    def __init__(
        self,
        # Input data
        data,

        #Input Hyper Parameters
        input_type='seven',

        # Arch Hyper Parameters
        layers="2,2",

        # Optim Hyperparameters
        base_lr=0.1,
        batch_size=100,
        scheduler="cosine",

        fold=0,
        **kwargs
    ):
        super().__init__()

        # What inputs?
        self.inputs = self.input_map[input_type]
        input_size = len(self.inputs)

        self.fold = fold
        self.data = data

        self.save_hyperparameters(ignore=["data"])

        # Build Architecture
        layers = list(map(int, layers.split(","))) ## Split on comma, and convert layers to ints
        layers.append(2) ## Add a output layer with two classes
        layers = [l for l in layers if l > 0] ## Not necessary but eliminates any 0 neuron layers just incase

        # Iteratively build a list of FC->ReLU
        nnLayers = []
        nnLayers += [nn.Linear(input_size, layers[0]), nn.ReLU()]
        nnLayers += sum(
            [
                [nn.Linear(layers[i], layers[i + 1]), nn.ReLU()]
                for i in range(len(layers) - 1)
            ],
            [],
        )
        self.layers = nn.Sequential(*nnLayers)

        # Loss
        self.ce = nn.CrossEntropyLoss()

        # Metrics Setup
        metrics = MetricCollection(
            [
                Accuracy(),
                Precision(),
                Recall(),
                Specificity(),
                AUROC(compute_on_step=False),
                AveragePrecision(),
            ]
        )

        # Optimization Hyperparams
        self.base_lr = base_lr
        self.batch_size = int(batch_size)
        self._scheduler = scheduler

        self.train_data, self.validation_data = self.get_split()
        self.train_metrics = metrics.clone(prefix="train/")
        self.valid_metrics = metrics.clone(prefix="val/")

    def get_split(self):
        keep = self.data["split"] == self.fold
        train = self.data[~keep]
        val = self.data[keep]
        return self.df_to_dataset(train), self.df_to_dataset(val)

    def df_to_dataset(self, data):
        X = data[self.inputs].values.astype(float)
        Y = data["y"].values.astype(float)
        tX = torch.Tensor(X)
        tY = torch.Tensor(Y).type(torch.LongTensor)
        return TensorDataset(tX, tY)

    def forward(self, x):
        return self.layers(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        print('Before', x.shape)
        x = x.view(x.size(0), -1)
        print('After', x.shape)
        y_hat = self.layers(x)
        loss = self.ce(y_hat, y)
        self.train_metrics.update(y_hat[:, 0], y.flatten())
        return {"loss": loss}

    def on_train_epoch_end(self):
        self.log_dict(self.train_metrics.compute(), on_epoch=True)

    def validation_step(self, batch, batch_idx):
        x, y = batch
        x = x.view(x.size(0), -1)
        y_hat = self.layers(x)
        loss = self.ce(y_hat, y)
        self.valid_metrics.update(y_hat[:, 0], y)
        return {"loss": loss}

    def validation_epoch_end(self, outputs):
        loss = torch.stack([x["loss"] for x in outputs]).mean()
        o = self.valid_metrics.compute()
        self.log_dict(o, on_epoch=True)
        self.log("val/loss", loss)
        self.log("hp_metric", o["val/AveragePrecision"])
        self.log("epoch", self.current_epoch)

    def configure_optimizers(self):
        # Optimizer
        optimizer = torch.optim.Adam(self.parameters(), lr=self.base_lr)
        epoch_len = len(self.train_data) // self.batch_size

        # Scheduler
        if self._scheduler == "cosine":
          lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer, epoch_len * 3
            )

        elif self._scheduler == "cyclic":
            lr_scheduler = torch.optim.lr_scheduler.CyclicLR(
                optimizer,
                base_lr=self.base_lr,
                max_lr=max(self.base_lr * 10, 0.1),
                cycle_momentum=False,
                step_size_up=epoch_len,
                step_size_down=2 * epoch_len,
            )

        ## No LR reduction
        else:
            lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1e100)

        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": lr_scheduler,
                "interval": "step",
                "frequency": 1,
            },
        }

    def train_dataloader(self):
        return DataLoader(self.train_data, batch_size=self.batch_size)

    def val_dataloader(self):
        return DataLoader(self.validation_data, batch_size=self.batch_size)

"""Now that we have a model built, write code to train your model on input data.  Remember to have an input that specifies the validation fold you wish to use!"""

import pytorch_lightning as pl
from pytorch_lightning import loggers as pl_loggers
from ray import tune
from ray.tune.integration.pytorch_lightning import TuneReportCallback
from ray.tune.suggest import Repeater
from ray.tune.suggest.hyperopt import HyperOptSearch


def tune_run(
    data,
    num_samples=50,
    num_epochs=15,
    name=f"tune_run",
    local_dir="./ray_tune",
    split_col="split",
):

    search_space = {
        "layers": tune.choice(["2",  "8", "16", "4,4", "4,8,4", "4,16", "16,8"]),
        "input_type": tune.choice(["seven", "all"]),
        "base_lr": tune.choice(np.geomspace(1e-9, 1, 10)[::2]),
        "batch_size": tune.choice([8, 512, 1048]),
        "scheduler": tune.choice(
            [
                "cyclic",
                "cosine",
                "none",
            ]
        ),
    }

    hopt = HyperOptSearch(metric="loss", mode="min")

    n_splits = data[split_col].nunique()
    searcher = Repeater(hopt, repeat=n_splits, set_index=True)

    # scheduler = ASHAScheduler(max_t=num_epochs, grace_period=5, reduction_factor=2)

    reporter = tune.JupyterNotebookReporter(
        True,
        parameter_columns=["layers", "base_lr", "batch_size", "scheduler", "input_type"],
        metric_columns=[
            "mean_recall",
            "mean_specificity",
            "mean_auroc",
            "loss",
            "mean_ap",
            "epoch",
        ],
        print_intermediate_tables=2,
    )

    analysis = tune.run(
        tune.with_parameters(train_tune, data=data, num_epochs=num_epochs, num_gpus=0),
        config=search_space,
        resources_per_trial={"cpu": 1, "gpu": 0},
        num_samples=num_samples,
        # scheduler=scheduler,
        search_alg=searcher,
        progress_reporter=reporter,
        name=name,
        local_dir=local_dir,
        metric="loss",
        mode="min",
        log_to_file=("output.log", "errors.log"),
    )

    print("Best hyperparameters found were: ", analysis.best_config)

def train_tune(config, data, num_epochs=15, num_gpus=0):

    config["fold"] = config.pop(tune.suggest.repeater.TRIAL_INDEX, 0)

    model = MLP(data, **config)
    trainer = pl.Trainer(
        max_epochs=num_epochs,
        gpus=0,
        logger=pl_loggers.TensorBoardLogger(
            save_dir=tune.get_trial_dir(), name="", version="."
        ),
        progress_bar_refresh_rate=0,
        num_sanity_val_steps=-1,
        callbacks=[
            TuneReportCallback(
                {
                    "mean_recall": "val/Recall",
                    "mean_specificity": "val/Specificity",
                    "mean_auroc": "val/AUROC",
                    "mean_ap": "val/AveragePrecision",
                    "loss": "val/loss",
                    "epoch": "epoch",
                },
                on="validation_end",
            ),
            LearningRateMonitor(logging_interval=None),
        ],
    )
    trainer.fit(model)

"""Now train your model and optionally tune your hyperparameters!

Tip: If you are logging to tensorboard, you can view your logs with this snippet (run in a separate cell before your training code):

```
%load_ext tensorboard
%tensorboard --logdir ./logs
```


"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir ./tune_logs

name = "sm"
num_samples = 6*4
# !rm -rf tune_logs
tune_run(trainval, num_samples=num_samples, name=name, local_dir="./tune_logs")

name = "lg"
num_samples = 600*4
tune_run(trainval, num_samples=num_samples, name=name, local_dir="./tune_logs")

